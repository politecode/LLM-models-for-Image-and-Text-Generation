{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyO5pPUE0KlSFe+5KFJCMU7n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/politecode/Llama-2-Chatbot/blob/main/LLAMA_2_Chat_App.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pm_DdEqU1lt8",
        "outputId": "80d289b2-1866-42aa-dea9-4f112d341d97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: replicate in /usr/local/lib/python3.10/dist-packages (0.22.0)\n",
            "Requirement already satisfied: httpx<1,>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from replicate) (0.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from replicate) (23.2)\n",
            "Requirement already satisfied: pydantic>1 in /usr/local/lib/python3.10/dist-packages (from replicate) (1.10.13)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from replicate) (4.5.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (1.0.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (1.3.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.21.0->replicate) (0.14.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.21.0->replicate) (1.2.0)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "! pip install replicate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "key = \"r8_0oERUSsx101iznA7gD5mP2S9K9AoKe53ls4LC\"\n",
        "os.environ[\"REPLICATE_API_TOKEN\"] = key"
      ],
      "metadata": {
        "id": "MsHjBeBz12D-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import replicate\n",
        "output = replicate.run(\n",
        "    \"stability-ai/stable-diffusion:db21e45d3f7023abc2a46ee38a23973f6dce16bb082a930b0c49861f96d1e5bf\",\n",
        "    input={\"text\": \"an player playing tennis\"}\n",
        ")\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGIl07hY9ih2",
        "outputId": "9aa6c3d3-3599-4941-d455-f3d674a0c6eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['https://replicate.delivery/pbxt/9WJZi0lrDF6JJFencj6w1IJCQewAsKoSPipti80zHiqn94BSA/out-0.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import replicate\n",
        "\n",
        "# https://replicate.com/meta/llama-2-70b-chat\n",
        "model_version = \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\"\n",
        "\n",
        "for event in replicate.stream(\n",
        "    model_version,\n",
        "    input={\n",
        "        \"prompt\": \"Please write a haiku about llamas.\",\n",
        "    },\n",
        "):\n",
        "    print(str(event), end=\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fu2YIeYEzTl",
        "outputId": "d35c84df-953f-496a-9b23-c78c86a3bb8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Sure, here's a haiku about llamas:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import replicate\n",
        "\n",
        "# Prompts\n",
        "pre_prompt = \"You are a helpful assistant. You do not respond as 'User' or pretend to be 'User'. You only respond once as 'Assistant'.\"\n",
        "prompt_input = \"How I can Learn LLms. Give Roadmap?\"\n",
        "\n",
        "# Generate LLM response\n",
        "output = replicate.run('a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5', # LLM model\n",
        "                        input={\"prompt\": f\"{pre_prompt} {prompt_input} Assistant: \", # Prompts\n",
        "                        \"temperature\":0.6, \"top_p\":0.9, \"max_length\":508, \"repetition_penalty\":1})  # Model par"
      ],
      "metadata": {
        "id": "tEE1oZMo12Ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4DgQC5C12R_",
        "outputId": "9bac8712-7d61-43ce-ce4f-4e86c4b7fd12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object Prediction.output_iterator at 0x7e3fbb309310>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_response = \"\"\n",
        "\n",
        "for item in output:\n",
        "  full_response += item\n",
        "\n",
        "print(full_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtMroBhd12bG",
        "outputId": "7971a9ab-ab66-47ce-fcb3-442fb11149fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! I'm happy to help you learn LLMs! To get started, I recommend checking out the following resources:\n",
            "\n",
            "1. The LLMS documentation: This is a comprehensive guide to all things LLMS, including installation, configuration, and usage.\n",
            "\n",
            "2. The LLMS community forum: This is a great place to ask questions and get help from other users and developers.\n",
            "\n",
            "3. The LLMS GitHub repository: This is where the source code for LLMS is stored, and it's a great place to explore if you're interested in contributing to the project or learning more about how it works under the hood.\n",
            "\n",
            "As for a roadmap, here are some steps you can follow to learn LLMs:\n",
            "\n",
            "1. Start with the basics: Begin by familiarizing yourself with the basic concepts of LLMs, such as the different types of LLMs, how they are used, and the benefits they provide.\n",
            "\n",
            "2. Explore the documentation: Dive deeper into the LLMS documentation to learn about the various features and capabilities of the platform.\n",
            "\n",
            "3. Join the community: Participate in the LLMS community forum to ask questions, share your experiences, and learn from others.\n",
            "\n",
            "4. Try it out: Set up a test environment and try out some of the features of LLMS to get a feel for how it works.\n",
            "\n",
            "5. Contribute: If you're interested in contributing to the project, start by exploring the LLMS GitHub repository and looking for issues or features that you could help with.\n",
            "\n",
            "I hope this helps! Let me know if you have any other questions.\n"
          ]
        }
      ]
    }
  ]
}